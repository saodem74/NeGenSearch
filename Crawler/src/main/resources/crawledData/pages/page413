Hardware acceleration From Wikipedia, the free encyclopedia Jump to navigation Jump to search For hardware accelerators in startup companies, see Seed accelerator . This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources   Hardware acceleration     news    newspapers    books    scholar    JSTOR  September 2014   Learn how and when to remove this template message  In computing , hardware acceleration is the use of computer hardware specially made to perform some functions more efficiently than is possible in software running on a general-purpose CPU . Any transformation of data or routine that can be computed , can be calculated purely in software running on a generic CPU, purely in custom-made hardware , or in some mix of both. An operation can be computed faster in application-specific hardware designed or programmed to compute the operation than specified in software and performed on a general-purpose computer processor . Each approach has advantages and disadvantages. The implementation of computing tasks in hardware to decrease latency and increase throughput is known as hardware acceleration . Typical advantages of software include more rapid development leading to faster times to market , lower non-recurring engineering costs, heightened portability , and ease of updating features or patching bugs , at the cost of overhead to compute general operations. Advantages of hardware include speedup , reduced power consumption , 1  lower latency , increased parallelism 2  and bandwidth , and better utilization of area and functional components available on an integrated circuit  at the cost of lower ability to update designs once etched onto silicon and higher costs of functional verification and times to market. In the hierarchy of digital computing systems ranging from general-purpose processors to fully customized hardware, there is a tradeoff between flexibility and efficiency, with efficiency increasing by orders of magnitude when any given application is implemented higher up that hierarchy. 3  4  This hierarchy includes general-purpose processors such as CPUs, more specialized processors such as GPUs, fixed-function implemented on field-programmable gate arrays FPGAs, and fixed-function implemented on application-specific integrated circuit ASICs. Hardware acceleration is advantageous for performance , and practical when the functions are fixed so updates are not as needed as in software solutions. With the advent of reprogrammable logic devices such as FPGAs, the restriction of hardware acceleration to fully fixed algorithms has eased since 2010, allowing hardware acceleration to be applied to problem domains requiring modification to algorithms and processing control flow . 5  6  7  Contents 1 Overview 1.1 Computational equivalence of hardware and software 1.2 Stored-program computers 1.3 Hardware execution units 1.4 Emerging hardware architectures 1.5 Implementation Metrics 2 Example tasks accelerated 2.1 Summing two arrays into a third array 2.2 Summing one million integers 2.3 Stream processing 3 Applications 4 Hardware acceleration units by application 5 See also 6 References 7 External links Overview  edit  Integrated circuits can be created to perform arbitrary operations on analog and digital signals. Most often in computing, signals are digital and can be interpreted as binary number data. Computer hardware and software operate on information in binary representation to perform computing  this is accomplished by calculating boolean functions on the bits of input and outputting the result to some output device downstream for storage or further processing . Computational equivalence of hardware and software  edit  Either software or hardware can compute any computable function . Custom hardware offers higher performance per watt for the same functions that can be specified in software. Hardware description languages HDLs such as Verilog and VHDL can model the same semantics as software and synthesize the design into a netlist that can be programmed to an FPGA or composed into logic gates of an application-specific integrated circuit . Stored-program computers  edit  The vast majority of software-based computing occurs on machines implementing the von Neumann architecture , collectively known as stored-program computers . Computer programs are stored as data and executed by processors , typically one or more CPU cores . Such processors must fetch and decode instructions as well as data operands from memory as part of the instruction cycle to execute the instructions constituting the software program. Relying on a common cache for code and data leads to the von Neumann bottleneck , a fundamental limitation on the throughput of software on processors implementing the von Neumann architecture. Even in the modified Harvard architecture , where instructions and data have separate caches in the memory hierarchy , there is overhead to decoding instruction opcodes and multiplexing available execution units on a microprocessor or microcontroller , leading to low circuit utilization . Intel s hyper-threading technology provides simultaneous multithreading by exploiting under-utilization of available processor functional units and instruction level parallelism between different hardware threads . Hardware execution units  edit  Hardware execution units do not in general rely on the von Neumann or modified Harvard architectures and do not need to perform the instruction fetch and decode steps of an instruction cycle and incur those stages overhead. If needed calculations are specified in a register transfer level RTL hardware design, the time and circuit area costs that would be incurred by instruction fetch and decoding stages can be reclaimed and put to other uses. This reclamation saves time, power and circuit area in computation. The reclaimed resources can be used for increased parallel computation, other functions, communication or memory, as well as increased inputoutput capabilities. This comes at the opportunity cost of less general-purpose utility. Emerging hardware architectures  edit  Greater RTL customization of hardware designs allows emerging architectures such as in-memory computing , transport triggered architectures TTA and networks-on-chip NoC to further benefit from increased locality of data to execution context, thereby reducing computing and communication latency between modules and functional units. Custom hardware is limited in parallel processing capability only by the area and logic blocks available on the integrated circuit die . 8  Therefore, hardware is much more free to offer massive parallelism than software on general-purpose processors, offering a possibility of implementing the parallel random-access machine PRAM model. It is common to build multicore and manycore processing units out of microprocessor IP core schematics on a single FPGA or ASIC. 9  10  11  12  13  Similarly, specialized functional units can be composed in parallel as in digital signal processing without being embedded in a processor IP core . Therefore, hardware acceleration is often employed for repetitive, fixed tasks involving little conditional branching , especially on large amounts of data. This is how Nvidia s CUDA line of GPUs are implemented. Implementation Metrics  edit  As device mobility has increased, the relative performance of specific acceleration protocols has required new metricizations, considering the characteristics such as physical hardware dimensions, power consumption and operations throughput. These can be summarized into three categories task efficiency, implementation efficiency, and flexibility. Appropriate metrics consider t he area of the hardware along with both the corresponding operations throughput and energy consumed. 14  Example tasks accelerated  edit  Summing two arrays into a third array  edit  This section is empty. You can help by adding to it .  October 2018  Summing one million integers  edit  Suppose we wish to compute the sum of 2 20  1 , 048 , 576 displaystyle 2201,048,576 integers . Assuming large integers are available as bignum large enough to hold the sum, this can be done in software by specifying here, in C  constexpr int N  20  constexpr int two_to_the_N  1   N  bignum array_sum  const std  array  int , two_to_the_N   ints   bignum result  0  for  std  size_t i  0  i  two_to_the_N  i    result  ints  i   return result   This algorithm runs in linear time , O  n  textstyle mathcal Oleftnright in Big O notation . In hardware, with sufficient area on chip , calculation can be parallelized to take only 20 time steps using the prefix sum algorithm. 15  The algorithm requires only logarithmic time , O  log  n  textstyle mathcal Oleftlog nright , and O  1  textstyle mathcal Oleft1right space as an in-place algorithm  parameter int N  20  parameter int two_to_the_N  1   N  function int array_sum  input int array  two_to_the_N  begin for  genvar i  0  i  N  i   begin for  genvar j  0  j  two_to_the_N  j   begin if  j   1   i  begin array  j   array  j   array  j -  1   i  end end end return array  two_to_the_N - 1  end endfunction This example takes advantage of the greater parallel resources available in application-specific hardware than most software and general-purpose computing paradigms and architectures . Stream processing  edit  This section needs expansion . You can help by adding to it .  October 2018  Hardware acceleration can be applied to stream processing . Applications  edit  Examples of hardware acceleration include bit blit acceleration functionality in graphics processing units GPUs, use of memristors for accelerating neural networks 16  and regular expression hardware acceleration for spam control in the server industry, intended to prevent regular expression denial of service ReDoS attacks. 17  The hardware that performs the acceleration may be part of a general-purpose CPU, or a separate unit. In the second case, it is referred to as a hardware accelerator , or often more specifically as a 3D accelerator , cryptographic accelerator , etc. Traditionally, processors were sequential  instructions are executed one by one, and were designed to run general purpose algorithms controlled by instruction fetch for example moving temporary results to and from a register file . Hardware accelerators improve the execution of a specific algorithm by allowing greater concurrency , having specific datapaths for their temporary variables , and reducing the overhead of instruction control in the fetch-decode-execute cycle . Modern processors are multi-core and often feature parallel single-instruction multiple data  SIMD  units. Even so, hardware acceleration still yields benefits. Hardware acceleration is suitable for any computation-intensive algorithm which is executed frequently in a task or program. Depending upon the granularity, hardware acceleration can vary from a small functional unit, to a large functional block like motion estimation in MPEG-2 . Hardware acceleration units by application  edit  Application Hardware accelerator Acronym Computer graphics General-purpose tasks Nvidia graphics cards Ray tracing Graphics processing unit General-purpose computing on GPU CUDA architecture Ray-tracing hardware GPU GPGPU CUDA RTX Digital signal processing Digital signal processor DSP Analog signal processing Field-programmable analog array Field-programmable RF FPAA FPRF Sound processing Sound card and sound card mixer NA Computer networking on a chip TCP Inputoutput Network processor and network interface controller Network on a chip TCP offload engine IO Acceleration Technology NPU and NIC NoC TCPOE or TOE IOAT or IOAT Cryptography Encryption ISA SSLTLS Attack Random number generation Cryptographic accelerator and secure cryptoprocessor Hardware-based encryption AES instruction set SSL acceleration Custom hardware attack Hardware random number generator NA Artificial intelligence Machine vision  computer vision Neural networks Brain simulation AI accelerator Vision processing unit Physical neural network Neuromorphic engineering NA VPU PNN NA Multilinear algebra Tensor processing unit TPU Physics simulation Physics processing unit PPU Regular expressions 17  Regular expression coprocessor NA Data compression 18  Data compression accelerator NA In-memory processing Network on a chip and Systolic array NoC NA Any computing task Computer hardware Field-programmable gate arrays 19  Application-specific integrated circuits 19  Complex programmable logic devices Systems-on-Chip Multi-processor system-on-chip Programmable system-on-chip HW sometimes FPGA ASIC CPLD SoC MPSoC PSoC See also  edit  Coprocessor DirectX Video Acceleration DXVA Direct memory access DMA High-level synthesis C to HDL Flow to HDL Soft microprocessor Flynns taxonomy of parallel computer architectures Single instruction, multiple data SIMD Single instruction, multiple threads SIMT Multiple instructions, multiple data MIMD Computer for operations with functions References  edit   Microsoft Supercharges Bing Search With Programmable Chips . WIRED . 16 June 2014. .mw-parser-output cite.citationfont-styleinherit.mw-parser-output .citation qquotes.mw-parser-output .citation .cs1-lock-free abackgroundurlupload.wikimedia.orgwikipediacommonsthumb665Lock-green.svg9px-Lock-green.svg.pngno-repeatbackground-positionright .1em center.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation . cs1-lock-registration abackgroundurlupload.wikimedia.orgwikipediacommonsthumbdd6Lock-gray-alt-2.svg9px-Lock-gray-alt-2.svg.pngno-repeatbackground-positionright .1em center.mw-parser-output .citation .cs1-lock-subscription abackgroundurlupload.wikimedia.orgwikipediacommonsthumbaaaLock-red-alt-2.svg9px-Lock-red-alt-2.svg.pngno-repeatbackground-po sitionright .1em center.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registrationcolor555.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration spanborder-bottom1px dottedcursorhelp.mw-parser-output .cs1-ws-icon abackgroundurlupload.wikimedia.orgwikipediacommonsthumb44cWikisource-logo.svg12px-Wikisource-logo.svg.pngno -repeatbackground-positionright .1em center.mw-parser-output code.cs1-codecolorinheritbackgroundinheritborderinheritpaddinginherit.mw-parser-output .cs1-hidden-errordisplaynonefont-size100.mw-parser-output .cs1-visible-errorfont-size100.mw-parser-output .cs1-maintdisplaynonecolor33aa33margin-left0.3em.mw-parser-output .cs1-subscription,.mw-parser-o utput .cs1-registration,.mw-parser-output .cs1-formatfont-size95.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-leftpadding-left0.2em.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-rightpadding-right0.2em  Embedded . Archived from the original on 2007-10-08 . Retrieved 2012-08-18 . Cite uses deprecated parameter deadurl  help  FPGA Architectures from A to Z by Clive Maxfield 2006  Mining hardware comparison - Bitcoin . Retrieved 17 July 2014 .  Non-specialized hardware comparison - Bitcoin . Retrieved 25 February 2014 .   A Survey of FPGA-based Accelerators for Convolutional Neural Networks , S. Mittal, NCAA, 2018  Morgan, Timothy Pricket 2014-09-03. How Microsoft Is Using FPGAs To Speed Up Bing Search . Enterprise Tech . Retrieved 2018-09-18 .  Project Catapult . Microsoft Research .  MicroBlaze Soft Processor Frequently Asked Questions Archived 2011-10-27 at the Wayback Machine  Istvn Vassnyi. Implementing processor arrays on FPGAs. 1998. 1  Zhoukun WANG and Omar HAMMAMI. A 24 Processors System on Chip FPGA Design with Network on Chip. 2  John Kent. Micro16 Array - A Simple CPU Array 3  Kit Eaton. 1,000 Core CPU Achieved Your Future Desktop Will Be a Supercomputer. 2011. 4  Scientists Squeeze Over 1,000 Cores onto One Chip. 2011. 5 Archived 2012-03-05 at the Wayback Machine  Kienle, Frank Wehn, Norbert Meyr, Heinrich December 2011. On Complexity, Energy- and Implementation-Efficiency of Channel Decoders. IEEE Transactions on Communications . 59 12 33013310. arXiv  1003.3792 . doi  10.1109tcomm.2011.092011.100157 . ISSN   0090-6778 .  Hillis, W. Daniel Steele, Jr., Guy L. December 1986. Data parallel algorithms. Communications of the ACM . 29 12 11701183. doi  10.11457902.7903 .   A Survey of ReRAM-based Architectures for Processing-in-memory and Neural Networks , S. Mittal, Machine Learning and Knowledge Extraction, 2018  a b Regular Expressions in hardware . Retrieved 17 July 2014 .  Compression Accelerators - Microsoft Research . Microsoft Research . Retrieved 2017-10-07 .  a b Farabet, Clment, et al.  Hardware accelerated convolutional neural networks for synthetic vision systems . Circuits and Systems ISCAS, Proceedings of 2010 IEEE International Symposium on. IEEE, 2010. External links  edit  Media related to Hardware acceleration at Wikimedia Commons v t e Hardware acceleration Theory Universal Turing machine Parallel computing Distributed computing Applications GPU GPGPU DirectX Audio Digital signal processing Hardware random number generation Artificial intelligence Cryptography TLS Machine vision Custom hardware attack scrypt Networking Implementations High-level synthesis C to HDL FPGA ASIC CPLD System on Chip Network on Chip Architectures Data flow Transport triggered Multicore Manycore Heterogeneous In-memory computing Systolic array Neuromorphic Related Programmable logic Processor design chronology Digital electronics Virtualization Hardware emulation Logic synthesis Embedded systems v t e Graphics processing unit GPU Adreno AMD Radeon Pro Instinct Apple Nvidia GeForce Quadro Tesla InfiniteReality Intel GT Mali PD7220 PowerVR RCP VideoCore Voodoo Architecture Compute kernel Fabrication CMOS FinFET MOSFET Graphics pipeline Geometry Vertex HDR rendering MAC Rasterisation Shading Ray-tracing SIMD SIMT Tessellation T L Tiled rendering Unified shader model Components Blitter Geometry processor Inputoutput memory management unit Render output unit Shader unit Stream processor Tensor unit Texture mapping unit Video display controller Video processing unit Memory DMA Framebuffer SGRAM GDDR GDDR3 GDDR4 GDDR5 GDDR6 HBM Memory bandwidth Memory controller Shared graphics memory Texture memory VRAM Form factor IP core Discrete graphics Clustering Switching External graphics Integrated graphics System on a chip Performance Clock rate Display resolution Fillrate Pixels Texels FLOPs Frame rate Performance per watt Transistor count Misc 2D Scrolling Sprite Tile 3D GI Texture ASIC GPGPU Graphics library Hardware acceleration Image processing Compression Parallel computing Vector processor Video coding Codec VLIW Retrieved from  httpsen.wikipedia.orgwindex.phptitleHardware_acceleration oldid910414651  Categories  Application-specific integrated circuits Central processing unit Computer optimization Gate arrays Graphics hardware Hardware acceleration Hidden categories CS1 errors deprecated parameters Webarchive template wayback links Articles needing additional references from September 2014 All articles needing additional references Articles to be expanded from October 2018 All articles to be expanded Articles with empty sections from October 2018 All articles with empty sections Articles using small message boxes Commons category link from Wikidata