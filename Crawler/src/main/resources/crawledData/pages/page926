Simplex algorithm From Wikipedia, the free encyclopedia Jump to navigation Jump to search This article is about the linear programming algorithm. For the non-linear optimization heuristic, see NelderMead method . In mathematical optimization , Dantzig s simplex algorithm or simplex method  is a popular algorithm for linear programming . 1  The name of the algorithm is derived from the concept of a simplex and was suggested by T. S. Motzkin . 2  Simplices are not actually used in the method, but one interpretation of it is that it operates on simplicial cones , and these become proper simplices with an additional constraint. 3  4  5  6  The simplicial cones in question are the corners i.e., the neighborhoods of the vertices of a geometric object called a polytope . The shape of this polytope is defined by the constraints applied to the objective function. Contents 1 Overview 2 History 3 Standard form 4 Simplex tableau 5 Pivot operations 6 Algorithm 6.1 Entering variable selection 6.2 Leaving variable selection 6.3 Example 7 Finding an initial canonical tableau 7.1 Example 8 Advanced topics 8.1 Implementation 8.2 Degeneracy stalling and cycling 8.3 Efficiency 9 Other algorithms 10 Linear-fractional programming 11 See also 12 Notes 13 References 14 Further reading 15 External links Overview  edit  Further information Linear programming A system of linear inequalities defines a polytope as a feasible region. The simplex algorithm begins at a starting vertex and moves along the edges of the polytope until it reaches the vertex of the optimal solution. Polyhedron of simplex algorithm in 3D The simplex algorithm operates on linear programs in the canonical form maximize c T x textstyle mathbf cT mathbf x  subject to A x  b displaystyle Amathbf x leq mathbf b  and x  0 displaystyle mathbf x geq 0 with x   x 1 ,  , x n  displaystyle mathbf x x_1,,dots ,,x_n the variables of the problem, c   c 1 ,  , c n  displaystyle mathbf c c_1,,dots ,,c_n the coefficients of the objective function, A displaystyle A a pn matrix, and b   b 1 ,  , b p  displaystyle mathbf b b_1,,dots ,,b_p nonnegative constants   j , b j  0   displaystyle forall j,b_jgeq 0  . There is a straightforward process to convert any linear program into one in standard form, so using this form of linear programs results in no loss of generality. In geometric terms, the feasible region defined by all values of x displaystyle mathbf x  such that A x  b textstyle Amathbf x leq mathbf b  and  i , x i  0 displaystyle forall i,x_igeq 0 is a possibly unbounded convex polytope . An extreme point or vertex of this polytope is known as basic feasible solution BFS. It can be shown that for a linear program in standard form, if the objective function has a maximum value on the feasible region, then it has this value on at least one of the extreme points. 7  This in itself reduces the problem to a finite computation since there is a finite number of extreme points, but the number of extreme points is unmanageably large for all but the smallest linear programs. 8  It can also be shown that, if an extreme point is not a maximum point of the objective function, then there is an edge containing the point so that the objective function is strictly increasing on the edge moving away from the point. 9  If the edge is finite, then the edge connects to another extreme point where the objective function has a greater value, otherwise the objective function is unbounded above on the edge and the linear program has no solution. The simplex algorithm applies this insight by walking along edges of the polytope to extreme points with greater and greater objective values. This contin ues until the maximum value is reached, or an unbounded edge is visited concluding that the problem has no solution. The algorithm always terminates because the number of vertices in the polytope is finite moreover since we jump between vertices always in the same direction that of the objective function, we hope that the number of vertices visited will be small. 9  The solution of a linear program is accomplished in two steps. In the first step, known as Phase I, a starting extreme point is found. Depending on the nature of the program this may be trivial, but in general it can be solved by applying the simplex algorithm to a modified version of the original program. The possible results of Phase I are either that a basic feasible solutio n is found or that the feasible region is empty. In the latter case the linear program is called infeasible . In the second step, Phase II, the simplex algorithm is applied using the basic feasible solution found in Phase I as a starting point. The possible results from Phase II are either an optimum basic feasible solution or an infinite edge on which the objective function is unbounded above. 10  11  12  History  edit  George Dantzig worked on planning methods for the US Army Air Force during World War II using a desk calculator. During 1946 his colleague challenged him to mechanize the planning process to distract him from taking another job. Dantzig formulated the problem as linear inequalities inspired by the work of Wassily Leontief , however, at that time he didnt include an objective as part of his formulation. Without an objective, a vast number of solutions can be feasible, and therefore to find the best feasible solution, military-specified ground rules must be used that describe how goals can be achieved as opposed to specifying a goal itself. Dantzigs core insight was to realize that most such ground rules can be translated into a linear objective function that needs to be maximized. 13  Development of the simplex method was evolutionary and happened over a period of about a year. 14  After Dantzig included an objective function as part of his formulation during mid-1947, the problem was mathematically more tractable. Dantzig realized that one of the unsolved problems that he had mistaken as homework in his professor Jerzy Neyman s class and actually later solved, was applicable to finding an algorithm for linear programs. This problem involved finding the existence of Lagrange multipliers for general linear programs over a continuum of variables, each bounded between zero and one, and satisfying linear constraints expressed in the form of Lebesgue integrals . Dantzig later published his homework as a thesis to earn his doctorate. The column geometry used in this thesis gave Dantzig insight that made him believe that the Simplex method would be very efficient. 15  Standard form  edit  The transformation of a linear program to one in standard form may be accomplished as follows. 16  First, for each variable with a lower bound other than 0, a new variable is introduced representing the difference between the variable and bound. The original variable can then be eliminated by substitution. For example, given the constraint x 1  5 displaystyle x_1geq 5 a new variable, y 1 displaystyle y_1 , is introduced with y 1  x 1  5 x 1  y 1  5 displaystyle beginalignedy_1x_1-5x_1y_15endaligned The second equation may be used to eliminate x 1 displaystyle x_1 from the linear program. In this way, all lower bound constraints may be changed to non-negativity restrictions. Second, for each remaining inequality constraint, a new variable, called a slack variable , is introduced to change the constraint to an equality constraint. This variable represents the difference between the two sides of the inequality and is assumed to be non-negative. For example, the inequalities x 2  2 x 3  3  x 4  3 x 5  2 displaystyle beginalignedx_22x_3 leq 3-x_43x_5 geq 2endaligned are replaced with x 2  2 x 3  s 1  3  x 4  3 x 5  s 2  2 s 1 , s 2  0 displaystyle beginalignedx_22x_3s_1 3-x_43x_5-s_2 2s_1,,s_2 geq 0endaligned It is much easier to perform algebraic manipulation on inequalities in this form. In inequalities where  appears such as the second one, some authors refer to the variable introduced as a surplus variable . Third, each unrestricted variable is eliminated from the linear program. This can be done in two ways, one is by solving for the variable in one of the equations in which it appears and then eliminating the variable by substitution. The other is to replace the variable with the difference of two restricted variables. For example, if z 1 displaystyle z_1 is unrestricted then write z 1  z 1   z 1  z 1  , z 1   0 displaystyle beginaligned z_1z_1-z_1- z_1,,z_1-geq 0endaligned The equation may be used to eliminate z 1 displaystyle z_1 from the linear program. When this process is complete the feasible region will be in the form A x  b ,  i   x i  0 displaystyle mathbf A mathbf x mathbf b ,,forall i x_igeq 0 It is also useful to assume that the rank of A displaystyle mathbf A  is the number of rows. This results in no loss of generality since otherwise either the system A x  b displaystyle mathbf A mathbf x mathbf b  has redundant equations which can be dropped, or the system is inconsistent and the linear program has no solution. 17  Simplex tableau  edit  A linear program in standard form can be represented as a tableau of the form  1  c T 0 0 A b  displaystyle beginbmatrix1 -mathbf c T 00 mathbf A mathbf b endbmatrix The first row defines the objective function and the remaining rows specify the constraints. The zero in the first column represents the zero vector of the same dimension as vector b .Different authors use different conventions as to the exact layout. If the columns of A can be rearranged so that it contains the identity matrix of order p the number of rows in A then the tableau is said to be in canonical form . 18  The variables corresponding to the columns of the identity matrix are called basic variables while the remaining variables are called nonbasic or free variables . If the values of the nonbasic variables are set to 0, then the values of the basic variables are easily obtained as entries in b and this solution is a basic feasible solution. The algebraic interpretation here is that the coefficients of the linear equation represented by each row are either 0 displaystyle 0 , 1 displaystyle 1 , or some other number. Each row will have 1 displaystyle 1 column with value 1 displaystyle 1 , p  1 displaystyle p-1 columns with coefficients 0 displaystyle 0 , and the remaining columns with some other coefficients these other variables represent our non-basic variables. By setting the values of the non-basic variables we ensure in each row that the value of the variable represented by a 1 displaystyle 1 in its column is equal to the b displaystyle b value at that row. Conversely, given a basic feasible solution, the columns corresponding to the nonzero variables can be expanded to a nonsingular matrix. If the corresponding tableau is multiplied by the inverse of this matrix then the result is a tableau in canonical form. 19  Let  1  c B T  c D T 0 0 I D b  displaystyle beginbmatrix1 -mathbf c _BT -mathbf c _DT 00 I mathbf D mathbf b endbmatrix be a tableau in canonical form. Additional row-addition transformations can be applied to remove the coefficients c T B   from the objective function. This process is called pricing out and results in a canonical tableau  1 0  c  D T z B 0 I D b  displaystyle beginbmatrix1 0 -bar mathbf c _DT z_B0 I mathbf D mathbf b endbmatrix where z B is the value of the objective function at the corresponding basic feasible solution. The updated coefficients, also known as relative cost coefficients , are the rates of change of the objective function with respect to the nonbasic variables. 11  Pivot operations  edit  The geometrical operation of moving from a basic feasible solution to an adjacent basic feasible solution is implemented as a pivot operation . First, a nonzero pivot element is selected in a nonbasic column. The row containing this element is multiplied by its reciprocal to change this element to 1, and then multiples of the row are added to the other rows to change the other entries in the column to 0. The result is that, if the pivot element is in row r , then the column becomes the r -th column of the identity matrix. The variable for this column is now a basic variable, replacing the variable which corresponded to the r -th column of the identity matrix before the operation. In effect, the variable corresponding to the pivot column enters the set of basic variables and is called the entering variable , and the variable being replaced leaves the set of basic variables and is called the leaving variable . The tableau is still in canonical form but with the set of basic variables changed by one element. 10  11  Algorithm  edit  Let a linear program be given by a canonical tableau. The simplex algorithm proceeds by performing successive pivot operations each of which give an improved basic feasible solution the choice of pivot element at each step is largely determined by the requirement that this pivot improves the solution. Entering variable selection  edit  Since the entering variable will, in general, increase from 0 to a positive number, the value of the objective function will decrease if the derivative of the objective function with respect to this variable is negative. Equivalently, the value of the objective function is decreased if the pivot column is selected so that the corresponding entry in the objective row of the tabl eau is positive. If there is more than one column so that the entry in the objective row is positive then the choice of which one to add to the set of basic variables is somewhat arbitrary and several entering variable choice rules 20  such as Devex algorithm 21  have been developed. If all the entries in the objective row are less than or equal to 0 then no choice of entering variable can be made and the solution is in fact optimal. It is easily seen to be optimal since the objective row now corresponds to an equation of the form z  x   z B  nonnegative terms corresponding to nonbasic variables displaystyle zmathbf x z_Btextnonnegative terms corresponding to nonbasic variables By changing the entering variable choice rule so that it selects a column where the entry in the objective row is negative, the algorithm is changed so that it finds the maximum of the objective function rather than the minimum. Leaving variable selection  edit  Once the pivot column has been selected, the choice of pivot row is largely determined by the requirement that the resulting solution be feasible. First, only positive entries in the pivot column are considered since this guarantees that the value of the entering variable will be nonnegative. If there are no positive entries in the pivot column then the entering variable can ta ke any nonnegative value with the solution remaining feasible. In this case the objective function is unbounded below and there is no minimum. Next, the pivot row must be selected so that all the other basic variables remain positive. A calculation shows that this occurs when the resulting value of the entering variable is at a minimum. In other words, if the pivot column is c , then the pivot row r is chosen so that b r  a r c displaystyle b_ra_rc, is the minimum over all r so that a rc  0. This is called the minimum ratio test . 20  If there is more than one row for which the minimum is achieved then a dropping variable choice rule 22  can be used to make the determination. Example  edit  See also Revised simplex algorithm   Numerical example Consider the linear program Minimize Z   2 x  3 y  4 z displaystyle Z-2x-3y-4z, Subject to 3 x  2 y  z  10 2 x  5 y  3 z  15 x , y , z  0 displaystyle beginaligned3x2yz leq 102x5y3z leq 15x,,y,,z geq 0endaligned With the addition of slack variables s and t , this is represented by the canonical tableau  1 2 3 4 0 0 0 0 3 2 1 1 0 10 0 2 5 3 0 1 15  displaystyle beginbmatrix1 2 3 4 0 0 00 3 2 1 1 0 100 2 5 3 0 1 15endbmatrix where columns 5 and 6 represent the basic variables s and t and the corresponding basic feasible solution is x  y  z  0 , s  10 , t  15. displaystyle xyz0,,s10,,t15. Columns 2, 3, and 4 can be selected as pivot columns, for this example column 4 is selected. The values of z resulting from the choice of rows 2 and 3 as pivot rows are 101    10 and 153    5 respectively. Of these the minimum is 5, so row 3 must be the pivot row. Performing the pivot produces  3  2  11 0 0  4  60 0 7 1 0 3  1 15 0 2 5 3 0 1 15  displaystyle beginbmatrix3 -2 -11 0 0 -4 -600 7 1 0 3 -1 150 2 5 3 0 1 15endbmatrix Now columns 4 and 5 represent the basic variables z and s and the corresponding basic feasible solution is x  y  t  0 , z  5 , s  5. displaystyle xyt0,,z5,,s5. For the next step, there are no positive entries in the objective row and in fact Z   60  2 x  11 y  4 t 3   20  2 x  11 y  4 t 3 displaystyle Ztfrac -602x11y4t3-20tfrac 2x11y4t3 so the minimum value of Z is   20. Finding an initial canonical tableau  edit  In general, a linear program will not be given in canonical form and an equivalent canonical tableau must be found before the simplex algorithm can start. This can be accomplished by the introduction of artificial variables . Columns of the identity matrix are added as column vectors for these variables. If the b value for a constraint equation is negative, the equation is negated before adding the identity matrix columns. This does not change the set of feasible solutions or the optimal solution, and it ensures that the slack variables will constitute an initial feasible solution. The new table au is in canonical form but it is not equivalent to the original problem. So a new objective function, equal to the sum of the artificial variables, is introduced and the simplex algorithm is applied to find the minimum the modified linear program is called the Phase  I problem. 23  The simplex algorithm applied to the Phase I problem must terminate with a minimum value for the new objective function since, being the sum of nonnegative variables, its value is bounded below by 0. If the minimum is 0 then the artificial variables can be eliminated from the resulting canonical tableau producing a canonical tableau equivalent to the original problem. The simpl ex algorithm can then be applied to find the solution this step is called Phase  II . If the minimum is positive then there is no feasible solution for the Phase I problem where the artificial variables are all zero. This implies that the feasible region for the original problem is empty, and so the original problem has no solution. 10  11  24  Example  edit  Consider the linear program Minimize Z   2 x  3 y  4 z displaystyle Z-2x-3y-4z, Subject to 3 x  2 y  z  10 2 x  5 y  3 z  15 x , y , z  0 displaystyle beginaligned3x2yz 102x5y3z 15x,,y,,z geq 0endaligned This is represented by the non-canonical tableau  1 2 3 4 0 0 3 2 1 10 0 2 5 3 15  displaystyle beginbmatrix1 2 3 4 00 3 2 1 100 2 5 3 15endbmatrix Introduce artificial variables u and v and objective function W     u     v , giving a new tableau  1 0 0 0 0  1  1 0 0 1 2 3 4 0 0 0 0 0 3 2 1 1 0 10 0 0 2 5 3 0 1 15  displaystyle beginbmatrix1 0 0 0 0 -1 -1 00 1 2 3 4 0 0 00 0 3 2 1 1 0 100 0 2 5 3 0 1 15endbmatrix The equation defining the original objective function is retained in anticipation of Phase II. By construction, u and v are both non-basic variables since they are part of the initial identity matrix. However, the objective function W currently assumes that u and v are both 0 . In order to adjust the objective function to be the correct value where u     10 and v     15 , add the third and fourth rows to the first row giving  1 0 5 7 4 0 0 25 0 1 2 3 4 0 0 0 0 0 3 2 1 1 0 10 0 0 2 5 3 0 1 15  displaystyle beginbmatrix1 0 5 7 4 0 0 250 1 2 3 4 0 0 00 0 3 2 1 1 0 100 0 2 5 3 0 1 15endbmatrix Select column 5 as a pivot column, so the pivot row must be row 4, and the updated tableau is  3 0 7 1 0 0  4 15 0 3  2  11 0 0  4  60 0 0 7 1 0 3  1 15 0 0 2 5 3 0 1 15  displaystyle beginbmatrix3 0 7 1 0 0 -4 150 3 -2 -11 0 0 -4 -600 0 7 1 0 3 -1 150 0 2 5 3 0 1 15endbmatrix Now select column 3 as a pivot column, for which row 3 must be the pivot row, to get  1 0 0 0 0  1  1 0 0 7 0  25 0 2  10  130 0 0 7 1 0 3  1 15 0 0 0 11 7  2 3 25  displaystyle beginbmatrix1 0 0 0 0 -1 -1 00 7 0 -25 0 2 -10 -1300 0 7 1 0 3 -1 150 0 0 11 7 -2 3 25endbmatrix The artificial variables are now 0 and they may be dropped giving a canonical tableau equivalent to the original problem  7 0  25 0  130 0 7 1 0 15 0 0 11 7 25  displaystyle beginbmatrix7 0 -25 0 -1300 7 1 0 150 0 11 7 25endbmatrix This is, fortuitously, already optimal and the optimum value for the original linear program is  1307. Advanced topics  edit  Implementation  edit  Main article Revised simplex algorithm The tableau form used above to describe the algorithm lends itself to an immediate implementation in which the tableau is maintained as a rectangular  m    1-by- m     n    1 array. It is straightforward to avoid storing the m explicit columns of the identity matrix that will occur within the tableau by virtue of B being a subset of the columns of  A ,   I . This implementation is referred to as the  standard simplex algorithm. The storage and computation overhead are such that the standard simplex method is a prohibitively expensive approach to solving large linear programming problems. In each simplex iteration, the only data required are the first row of the tableau, the pivotal column of the tableau corresponding to the entering variable and the right-hand-side. The latter can be updated using the pivotal column and the first row of the tableau can be updated using the pivotal row corresponding to the leaving variable. Both the pivotal column and pivota l row may be computed directly using the solutions of linear systems of equations involving the matrix B and a matrix-vector product using A . These observations motivate the  revised simplex algorithm , for which implementations are distinguished by their invertible representation of   B . 25  In large linear-programming problems A is typically a sparse matrix and, when the resulting sparsity of B is exploited when maintaining its invertible representation, the revised simplex algorithm is much more efficient than the standard simplex method. Commercial simplex solvers are based on the revised simplex algorithm. 24  25  26  27  28  Degeneracy stalling and cycling  edit  If the values of all basic variables are strictly positive, then a pivot must result in an improvement in the objective value. When this is always the case no set of basic variables occurs twice and the simplex algorithm must terminate after a finite number of steps. Basic feasible solutions where at least one of the basic variables is zero are called degenerate and may result in pivots for which there is no improvement in the objective value. In this case there is no actual change in the solution but only a change in the set of basic variables. When several such pivots occur in succession, there is no improvement in large industrial applications, degeneracy is common and such  stalling  is notable. Worse than stalling is the possibility the same set of basic variables occurs twice, in which case, the deterministic pivoting rules of the simplex algorithm will produce an infinite loop, or cycle. While degeneracy is the rule in practice and stalling is common, cycling is rare in practice. A discussion of an example of practical cycling occurs in Padberg . 24  Blands rule prevents cycling and thus guarantees that the simplex algorithm always terminates. 24  29  30  Another pivoting algorithm, the criss-cross algorithm never cycles on linear programs. 31  History-based pivot rules such as Zadehs rule and Cunninghams rule also try to circumvent the issue of stalling and cycling by keeping track how often particular variables are being used, and then favor such variables that have been used least often. Efficiency  edit  The simplex method is remarkably efficient in practice and was a great improvement over earlier methods such as FourierMotzkin elimination . However, in 1972, Klee and Minty 32  gave an example, the KleeMinty cube , showing that the worst-case complexity of simplex method as formulated by Dantzig is exponential time . Since then, for almost every variation on the method, it has been shown that there is a family of linear programs for which it performs badly. It is an open question if there is a variation with polynomial time , although sub-exponential pivot rules are known. 33  In 2018, it was proved that a particular variant of the simplex method is NP-mighty , i.e., it can be used to solve, with polynomial overhead, any problem in NP implicitly during the algorithms execution. Moreover, deciding whether a given variable ever enters the basis during the algorithms execution on a given input, and determining the number of iterations needed for solving a given problem, are both NP-hard problems. 34  Computing the output of some other pivot rules was already known to be PSPACE-complete 35  36  . Analyzing and quantifying the observation that the simplex algorithm is efficient in practice despite its exponential worst-case complexity has led to the development of other measures of complexity. The simplex algorithm has polynomial-time average-case complexity under various probability distributions , with the precise average-case performance of the simplex algorithm depending on the choice of a probability distribution for the random matrices . 37  38  Another approach to studying  typical phenomena  uses Baire category theory from general topology , and to show that topologically most matrices can be solved by the simplex algorithm in a polynomial number of steps.  citation needed  Another method to analyze the performance of the simplex algorithm studies the behavior of worst-case scenarios under small perturbation  are worst-case scenarios stable under a small change in the sense of structural stability , or do they become tractable This area of research, called smoothed analysis , was introduced specifically to study the simplex method. Indeed, the running time of the simplex method on input with noise is polynomial in the number of variables and the magnitude of the perturbations. 39  Other algorithms  edit  Other algorithms for solving linear-programming problems are described in the linear-programming article. Another basis-exchange pivoting algorithm is the criss-cross algorithm . 40  41  There are polynomial-time algorithms for linear programming that use interior point methods these include Khachiyan s ellipsoidal algorithm , Karmarkar s projective algorithm , and path-following algorithms . 12  Linear-fractional programming  edit  Main article Linear-fractional programming Linearfractional programming LFP is a generalization of linear programming LP. In LP the objective function is a linear function , while the objective function of a linearfractional program is a ratio of two linear functions. In other words, a linear program is a fractionallinear program in which the denominator is the constant function having the value one everywhere. A linearfractional program can be solved by a variant of the simplex algorithm 42  43  44  45  or by the criss-cross algorithm . 46  See also  edit  Criss-cross algorithm Cutting-plane method Devex algorithm FourierMotzkin elimination Karmarkars algorithm NelderMead simplicial heuristic Pivoting rule of Bland , which avoids cycling Notes  edit   Murty, Katta G. Linear programming . John Wiley  Sons Inc.1, 2000. .mw-parser-output cite.citationfont-styleinherit.mw-parser-output .citation qquotes.mw-parser-output .citation .cs1-lock-free abackgroundurlupload.wikimedia.orgwikipediacommonsthumb665Lock-green.svg9px-Lock-green.svg.pngno-repeatbackground-positionright .1em center.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation . cs1-lock-registration abackgroundurlupload.wikimedia.orgwikipediacommonsthumbdd6Lock-gray-alt-2.svg9px-Lock-gray-alt-2.svg.pngno-repeatbackground-positionright .1em center.mw-parser-output .citation .cs1-lock-subscription abackgroundurlupload.wikimedia.orgwikipediacommonsthumbaaaLock-red-alt-2.svg9px-Lock-red-alt-2.svg.pngno-repeatbackground-po sitionright .1em center.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registrationcolor555.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration spanborder-bottom1px dottedcursorhelp.mw-parser-output .cs1-ws-icon abackgroundurlupload.wikimedia.orgwikipediacommonsthumb44cWikisource-logo.svg12px-Wikisource-logo.svg.pngno -repeatbackground-positionright .1em center.mw-parser-output code.cs1-codecolorinheritbackgroundinheritborderinheritpaddinginherit.mw-parser-output .cs1-hidden-errordisplaynonefont-size100.mw-parser-output .cs1-visible-errorfont-size100.mw-parser-output .cs1-maintdisplaynonecolor33aa33margin-left0.3em.mw-parser-output .cs1-subscription,.mw-parser-o utput .cs1-registration,.mw-parser-output .cs1-formatfont-size95.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-leftpadding-left0.2em.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-rightpadding-right0.2em  Murty 1983 , Comment 2.2  Murty 1983 , Note 3.9  Stone, Richard E. Tovey, Craig A. 1991. The simplex and projective scaling algorithms as iteratively reweighted least squares methods. SIAM Review . 33 2 220237. doi  10.11371033049 . JSTOR   2031142 . MR   1124362 .  Stone, Richard E. Tovey, Craig A. 1991. Erratum The simplex and projective scaling algorithms as iteratively reweighted least squares methods . SIAM Review . 33 3 461. doi  10.11371033100 . JSTOR   2031443 . MR   1124362 .  Strang, Gilbert 1 June 1987. Karmarkars algorithm and its place in applied mathematics. The Mathematical Intelligencer . 9 2 410. doi  10.1007BF03025891 . ISSN   0343-6993 . MR   0883185 .  Murty 1983 , Theorem 3.3  Murty 1983 , p.  143, Section 3.13  a b Murty 1983 , p.  137, Section 3.8  a b c George B. Dantzig and Mukund N. Thapa. 1997. Linear programming 1 Introduction . Springer-Verlag.  a b c d Evar D. Nering and Albert W. Tucker , 1993, Linear Programs and Related Problems , Academic Press. elementary  a b Robert J. Vanderbei, Linear Programming Foundations and Extensions , 3rd ed., International Series in Operations Research  Management Science, Vol. 114, Springer Verlag, 2008. ISBN   978-0-387-74387-5 .  Dantzig, George B. April 1982. Reminiscences about the origins of linear programming . Operations Research Letters . 1 2 4348. doi  10.10160167-63778290043-8 .  Albers and Reid 1986. An Interview with George B. Dantzig The Father of Linear Programming . College Mathematics Journal  292314.  Dantzig, George May 1987. Origins of the simplex method PDF . A History of Scientific Computing . doi  10.114587252.88081 inactive 2019-08-20. ISBN   978-0-201-50814-7 .  Murty 1983 , Section 2.2  Murty 1983 , p.  173  Murty 1983 , section 2.3.2  Murty 1983 , section 3.12  a b Murty 1983 , p.  66  Harris, Paula MJ. Pivot selection methods of the Devex LP code. Mathematical programming 5.1 1973 128  Murty 1983 , p.  67  Murty 1983 , p.  60  a b c d M. Padberg, Linear Optimization and Extensions , Second Edition, Springer-Verlag, 1999.  a b George B. Dantzig and Mukund N. Thapa. 2003. Linear Programming 2 Theory and Extensions . Springer-Verlag.  Dmitris Alevras and Manfred W. Padberg, Linear Optimization and Extensions Problems and Extensions , Universitext, Springer-Verlag, 2001. Problems from Padberg with solutions.  Maros, Istvn Mitra, Gautam 1996. Simplex algorithms. In J. E. Beasley ed.. Advances in linear and integer programming . Oxford Science. pp.  146. MR   1438309 .  Maros, Istvn 2003. Computational techniques of the simplex method . International Series in Operations Research  Management Science. 61 . Boston, MA Kluwer Academic Publishers. pp.  xx325. ISBN   978-1-4020-7332-8 . MR   1960274 .  Bland, Robert G. May 1977. New finite pivoting rules for the simplex method. Mathematics of Operations Research . 2 2 103107. doi  10.1287moor.2.2.103 . JSTOR   3689647 . MR   0459599 .  Murty 1983 , p.  79  There are abstract optimization problems, called oriented matroid programs, on which Blands rule cycles incorrectly while the criss-cross algorithm terminates correctly.  Klee, Victor  Minty, George J. 1972. How good is the simplex algorithm. In Shisha, Oved ed.. Inequalities III Proceedings of the Third Symposium on Inequalities held at the University of California, Los Angeles, Calif., September 19, 1969, dedicated to the memory of Theodore S. Motzkin . New York-London Academic Press. pp.  159175. MR   0332165 .  Hansen, Thomas Zwick, Uri 2015, An Improved Version of the Random-Facet Pivoting Rule for the Simplex Algorithm, Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing  209218, CiteSeerX   10.1.1.697.2526 , doi  10.11452746539.2746557 , ISBN   9781450335362  Disser, Yann Skutella, Martin 2018-11-01. The Simplex Algorithm Is NP-Mighty. ACM Trans. Algorithms . 15 1 51519. arXiv  1311.5935 . doi  10.11453280847 . ISSN   1549-6325 .  Adler, Ilan Christos, Papadimitriou  Rubinstein, Aviad 2014, On Simplex Pivoting Rules and Complexity Theory, International Conference on Integer Programming and Combinatorial Optimization , Lecture Notes in Computer Science, 17  1324, arXiv  1404.3320 , doi  10.1007978-3-319-07557-0_2 , ISBN   978-3-319-07556-3  Fearnly, John Savani, Rahul 2015, The Complexity of the Simplex Method, Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing  201208, arXiv  1404.0605 , doi  10.11452746539.2746558 , ISBN   9781450335362  Alexander Schrijver , Theory of Linear and Integer Programming . John Wiley  sons, 1998, ISBN   0-471-98232-6 mathematical  The simplex algorithm takes on average D steps for a cube. Borgwardt 1987  Borgwardt, Karl-Heinz 1987. The simplex method A probabilistic analysis . Algorithms and Combinatorics Study and Research Texts. 1 . Berlin Springer-Verlag. pp.  xii268. ISBN   978-3-540-17096-9 . MR   0868467 .  Spielman, Daniel Teng, Shang-Hua 2001. Smoothed analysis of algorithms why the simplex algorithm usually takes polynomial time. Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing . ACM. pp.  296305. arXiv  cs0111050 . doi  10.1145380752.380813 . ISBN   978-1-58113-349-3 .  Terlaky, Tams Zhang, Shu Zhong 1993. Pivot rules for linear programming A Survey on recent theoretical developments. Annals of Operations Research . 4647 1 203233. CiteSeerX   10.1.1.36.7658 . doi  10.1007BF02096264 . ISSN   0254-5330 . MR   1260019 .  Fukuda, Komei Terlaky, Tams 1997. Thomas  M. Liebling and Dominique de  Werra eds.. Criss-cross methods A fresh view on pivot algorithms. Mathematical Programming, Series B . 79 13. Amsterdam North-Holland Publishing  Co. pp.  369395. doi  10.1007BF02614325 . MR   1464775 . CS1 maint uses editors parameter  link   Murty 1983 , Chapter 3.20 pp. 160164 and pp. 168 and 179  Chapter five Craven, B. D. 1988. Fractional programming . Sigma Series in Applied Mathematics. 4 . Berlin Heldermann Verlag. p.  145. ISBN   978-3-88538-404-5 . MR   0949209 .  Kruk, Serge Wolkowicz, Henry 1999. Pseudolinear programming. SIAM Review . 41 4 795805. Bibcode  1999SIAMR..41..795K . CiteSeerX   10.1.1.53.7355 . doi  10.1137S0036144598335259 . JSTOR   2653207 . MR   1723002 .  Mathis, Frank H. Mathis, Lenora Jane 1995. A nonlinear programming algorithm for hospital management. SIAM Review . 37 2 230234. doi  10.11371037046 . JSTOR   2132826 . MR   1343214 .  Ills, Tibor Szirmai, kos Terlaky, Tams 1999. The finite criss-cross method for hyperbolic programming. European Journal of Operational Research . 114 1 198214. CiteSeerX   10.1.1.36.7090 . doi  10.1016S0377-22179800049-6 . ISSN   0377-2217 . PDF preprint . References  edit  Murty, Katta G. 1983. Linear programming . New York John Wiley  Sons, Inc. pp.  xix482. ISBN   978-0-471-09725-9 . MR   0720547 . Further reading  edit  These introductions are written for students of computer science and operations research  Thomas H. Cormen , Charles E. Leiserson , Ronald L. Rivest , and Clifford Stein . Introduction to Algorithms , Second Edition. MIT Press and McGraw-Hill, 2001. ISBN   0-262-03293-7 . Section 29.3 The simplex algorithm, pp.  790 804. Frederick S. Hillier and Gerald J. Lieberman Introduction to Operations Research , 8th edition. McGraw-Hill. ISBN   0-07-123828-X Rardin, Ronald L. 1997. Optimization in operations research . Prentice Hall. p.  919. ISBN   978-0-02-398415-0 . External links  edit  The Wikibook Operations Research has a page on the topic of The Simplex Method An Introduction to Linear Programming and the Simplex Algorithm by Spyros Reveliotis of the Georgia Institute of Technology. Greenberg, Harvey J., KleeMinty Polytope Shows Exponential Time Complexity of Simplex Method University of Colorado at Denver 1997 PDF download Simplex Method A tutorial for Simplex Method with examples also two-phase and M-method. Mathstools Simplex Calculator from www.mathstools.com Example of Simplex Procedure for a Standard Linear Programming Problem by Thomas McFarland of the University of Wisconsin-Whitewater. PHPSimplex online tool to solve Linear Programming Problems by Daniel Izquierdo and Juan Jos Ruiz of the University of Mlaga UMA, Spain simplex-m Online Simplex Solver v t e Optimization  Algorithms , methods , and heuristics Unconstrained nonlinear  functions Golden-section search Interpolation methods Line search NelderMead method Successive parabolic interpolation  and gradients Convergence Trust region Wolfe conditions QuasiNewton BerndtHallHallHausman BroydenFletcherGoldfarbShanno and L-BFGS DavidonFletcherPowell Symmetric rank-one SR1 Other methods GaussNewton Gradient LevenbergMarquardt Conjugate gradient Truncated Newton  and Hessians Newtons method Constrained nonlinear General Barrier methods Penalty methods Differentiable Augmented Lagrangian methods Sequential quadratic programming Successive linear programming Convex optimization Convex minimization Cutting-plane method Reduced gradient FrankWolfe Subgradient method Linear and quadratic Interior point Affine scaling Ellipsoid algorithm of Khachiyan Projective algorithm of Karmarkar Basis- exchange Simplex algorithm of Dantzig Revised simplex algorithm Criss-cross algorithm Principal pivoting algorithm of Lemke Combinatorial Paradigms Approximation algorithm Dynamic programming Greedy algorithm Integer programming Branch and bound  cut Graph algorithms Minimum spanning tree BellmanFord Borvka Dijkstra FloydWarshall Johnson Kruskal Network flows Dinic EdmondsKarp FordFulkerson Pushrelabel maximum flow Metaheuristics Evolutionary algorithm Hill climbing Local search Simulated annealing Tabu search Software v t e Complementarity problems and algorithms Complementarity Problems Linear programming LP Quadratic programming QP Linear complementarity problem LCP Mixed linear MLCP Mixed MCP NonlinearNCP Basis - exchange algorithms Simplex  Dantzig  Revised simplex Criss-cross Lemke Retrieved from  httpsen.wikipedia.orgwindex.phptitleSimplex_algorithm oldid914162226  Categories  Optimization algorithms and methods 1947 in computer science Exchange algorithms Linear programming Computer-related introductions in 1947 Hidden categories Pages with DOIs inactive as of 2019 August CS1 long volume value CS1 maint uses editors parameter All articles with unsourced statements Articles with unsourced statements from June 2019