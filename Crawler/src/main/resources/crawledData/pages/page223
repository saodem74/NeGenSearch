Concurrent computing From Wikipedia, the free encyclopedia Jump to navigation Jump to search For a more theoretical discussion, see Concurrency computer science . Programming paradigms Action Agent-oriented Array-oriented Automata-based Concurrent computing Relativistic programming Data-driven Declarative contrast Imperative  Functional Functional logic Purely functional Logic Abductive logic Answer set Concurrent logic Functional logic Inductive logic Constraint Constraint logic Concurrent constraint logic Dataflow Flow-based Reactive Ontology Differentiable Dynamicscripting Event-driven Function-level contrast Value-level  Point-free style Concatenative Generic Imperative contrast Declarative  Procedural Object-oriented Polymorphic Intentional Language-oriented Domain-specific Literate Natural-language programming Metaprogramming Automatic Inductive programming Reflective Attribute-oriented Macro Template Non-structured contrast Structured  Array Nondeterministic Parallel computing Process-oriented Probabilistic Quantum Stack-based Structured contrast Non-structured  Block-structured Object-oriented Actor-based Class-based Concurrent Prototype-based By separation of concerns  Aspect-oriented Role-oriented Subject-oriented Recursive Symbolic Value-level contrast Function-level  v t e Concurrent computing is a form of computing in which several computations are executed during overlapping time periods concurrently instead of sequentially one completing before the next starts. This is a property of a systemthis may be an individual program , a computer , or a network and there is a separate execution point or thread of control for each computation process. A concurrent system is one where a computation can advance without waiting for all other computations to complete. 1  As a programming paradigm , concurrent computing is a form of modular programming , namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra , Per Brinch Hansen , and C.A.R. Hoare . Contents 1 Introduction 1.1 Coordinating access to shared resources 1.2 Advantages 2 Models 3 Implementation 3.1 Interaction and communication 4 History 5 Prevalence 6 Languages supporting concurrent programming 7 See also 8 Notes 9 References 10 Sources 11 Further reading 12 External links Introduction  edit  See also Parallel computing This section has multiple issues. Please help improve it or discuss these issues on the talk page .  Learn how and when to remove these template messages  This section needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources   Concurrent computing     news    newspapers    books    scholar    JSTOR  December 2016   Learn how and when to remove this template message  This section possibly contains original research . Please improve it by verifying the claims made and adding inline citations . Statements consisting only of original research should be removed.  December 2016   Learn how and when to remove this template message   Learn how and when to remove this template message  The concept of concurrent computing is frequently confused with the related but distinct concept of parallel computing , 2  3  although both can be described as multiple processes executing during the same period of time . In parallel computing, execution occurs at the same physical instant for example, on separate processors of a multi-processor machine, with the goal of speeding up computationsparallel computing is impossible on a  one-core  single processor, as only one computation can occur at any instant during any single clock cycle. a  By contrast, concurrent computing consists of process lifetimes overlapping, but execution need not happen at the same instant. The goal here is to model processes in the outside world that happen concurrently, such as multiple clients accessing a server at the same time. Structuring software systems as composed of multiple concurrent, communicating parts can be useful for tackling complexity, regardless of whether the parts can be execute d in parallel. 4   1 For example, concurrent processes can be executed on one core by interleaving the execution steps of each process via time-sharing slices only one process runs at a time, and if it does not complete during its time slice, it is paused , another process begins or resumes, and then later the original process is resumed. In this way, multiple processes are part-way through execution at a single instant, but only one process is being executed at that instant.  citation needed  Concurrent computations may be executed in parallel, 2  5  for example, by assigning each process to a separate processor or processor core, or distributing a computation across a network. In general, however, the languages, tools, and techniques for parallel programming might not be suitable for concurrent programming, and vice versa.  citation needed  The exact timing of when tasks in a concurrent system are executed depend on the scheduling , and tasks need not always be executed concurrently. For example, given two tasks, T1 and T2  citation needed  T1 may be executed and finished before T2 or vice versa serial and sequential T1 and T2 may be executed alternately serial and concurrent T1 and T2 may be executed simultaneously at the same instant of time parallel and concurrent The word sequential is used as an antonym for both concurrent and parallel when these are explicitly distinguished, concurrentsequential and parallelserial are used as opposing pairs. 6  A schedule in which tasks execute one at a time serially, no parallelism, without interleaving sequentially, no concurrency no task begins until the prior task ends is called a serial schedule . A set of tasks that can be scheduled serially is serializable , which simplifies concurrency control .  citation needed  Coordinating access to shared resources  edit  The main challenge in designing concurrent programs is concurrency control  ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions. 5  Potential problems include race conditions , deadlocks , and resource starvation . For example, consider the following algorithm to make withdrawals from a checking account represented by the shared resource balance  1 bool withdraw  int withdrawal  2  3 if  balance  withdrawal  4  5 balance - withdrawal  6 return true  7  8 return false  9  Suppose balance  500 , and two concurrent threads make the calls withdraw300 and withdraw350 . If line 3 in both operations executes before line 5 both operations will find that balance  withdrawal evaluates to true , and execution will proceed to subtracting the withdrawal amount. However, since both processes perform their withdrawals, the total amount withdrawn will end up being more than the original balance. These sorts of problems with shared resources benefit from the use of concurrency control , or non-blocking algorithms . Advantages  edit  This section does not cite any sources . Please help improve this section by adding citations to reliable sources . Unsourced material may be challenged and removed .  December 2006   Learn how and when to remove this template message  Concurrent computing has the following advantages Increased program throughputparallel execution of a concurrent program allows the number of tasks completed in a given time to increase proportionally to the number of processors according to Gustafsons law High responsiveness for inputoutputinputoutput-intensive programs mostly wait for input or output operations to complete. Concurrent programming allows the time that would be spent waiting to be used for another task.  citation needed  More appropriate program structuresome problems and problem domains are well-suited to representation as concurrent tasks or processes.  citation needed  Models  edit  There are several models of concurrent computing, which can be used to understand and analyze concurrent systems. These models include Actor model Object-capability model for security Petri nets Process calculi such as Ambient calculus Calculus of communicating systems CCS Communicating sequential processes CSP -calculus Join-calculus Inputoutput automaton Preemptive machine scheduling Implementation  edit  This section needs expansion . You can help by adding to it .  February 2014  A number of different methods can be used to implement concurrent programs, such as implementing each computational execution as an operating system process , or implementing the computational processes as a set of threads within a single operating system process. Interaction and communication  edit  In some concurrent computing systems, communication between the concurrent components is hidden from the programmer e.g., by using futures , while in others it must be handled explicitly. Explicit communication can be divided into two classes Shared memory communication Concurrent components communicate by altering the contents of shared memory locations exemplified by Java and C . This style of concurrent programming usually needs the use of some form of locking e.g., mutexes , semaphores , or monitors  to coordinate between threads. A program that properly implements any of these is said to be thread-safe . Message passing communication Concurrent components communicate by exchanging messages exemplified by MPI , Go , Scala , Erlang and occam . The exchange of messages may be carried out asynchronously, or may use a synchronous rendezvous style in which the sender blocks until the message is received. Asynchronous message passing may be reliable or unreliable sometimes referred to as send and pray. Message-passing concurrency tends to be far easier to reason about than shared-memory concurrency, and is typica lly considered a more robust form of concurrent programming.  citation needed  A wide variety of mathematical theories to understand and analyze message-passing systems are available, including the actor model , and various process calculi . Message passing can be efficiently implemented via symmetric multiprocessing , with or without shared memory cache coherence . Shared memory and message passing concurrency have different performance characteristics. Typically although not always, the per-process memory overhead and task switching overhead is lower in a message passing system, but the overhead of message passing is greater than for a procedure call. These differences are often overwhelmed by other performance factors. History  edit  Concurrent computing developed out of earlier work on railroads and telegraphy , from the 19th and early 20th century, and some terms date to this period, such as semaphores. These arose to address the question of how to handle multiple trains on the same railroad system avoiding collisions and maximizing efficiency and how to handle multiple transmissions over a given set of wires improving efficiency, such as via time-division multiplexing 1870s. The academic study of concurrent algorithms started in the 1960s, with Dijkstra 1965 credited with being the first paper in this field, identifying and solving mutual exclusion . 7  Prevalence  edit  Concurrency is pervasive in computing, occurring from low-level hardware on a single chip to worldwide networks. Examples follow. At the programming language level Channel Coroutine Futures and promises At the operating system level Computer multitasking , including both cooperative multitasking and preemptive multitasking Time-sharing , which replaced sequential batch processing of jobs with concurrent use of a system Process Thread At the network level, networked systems are generally concurrent by their nature, as they consist of separate devices. Languages supporting concurrent programming  edit  Concurrent programming languages are programming languages that use language constructs for concurrency . These constructs may involve multi-threading , support for distributed computing , message passing , shared resources including shared memory  or futures and promises . Such languages are sometimes described as concurrency-oriented languages or concurrency-oriented programming languages COPL. 8  Today, the most commonly used programming languages that have specific constructs for concurrency are Java and C . Both of these languages fundamentally use a shared-memory concurrency model, with locking provided by monitors although message-passing models can and have been implemented on top of the underlying shared-memory model. Of the languages that use a message-passing concurrency model, Erlang is probably the most widely used in industry at present.  citation needed  Many concurrent programming languages have been developed more as research languages e.g. Pict  rather than as languages for production use. However, languages such as Erlang , Limbo , and occam have seen industrial use at various times in the last 20 years. Languages in which concurrency plays an important role include Ada general purpose, with native support for message passing and monitor based concurrency Alef concurrent, with threads and message passing, for system programming in early versions of Plan 9 from Bell Labs Alice extension to Standard ML , adds support for concurrency via futures Ateji PX extension to Java with parallel primitives inspired from -calculus Axum domain specific, concurrent, based on actor model and .NET Common Language Runtime using a C-like syntax BMDFM Binary Modular DataFlow Machine C stdthread Cpp-Taskflow Modern C task-based parallel programming library C C omegafor research, extends C, uses asynchronous communication C supports concurrent computing using lock, yield, also since version 5.0 async and await keywords introduced Clojure modern, functional dialect of Lisp on the Java platform Concurrent Clean functional programming, similar to Haskell Concurrent Collections CnCAchieves implicit parallelism independent of memory model by explicitly defining flow of data and control Concurrent Haskell lazy, pure functional language operating concurrent processes on shared memory Concurrent ML concurrent extension of Standard ML Concurrent Pascal by Per Brinch Hansen Curry D  multi-paradigm system programming language with explicit support for concurrent programming  actor model  E uses promises to preclude deadlocks ECMAScript uses promises for asynchronous operations Eiffel through its SCOOP mechanism based on the concepts of Design by Contract Elixir dynamic and functional meta-programming aware language running on the Erlang VM. Erlang uses asynchronous message passing with nothing shared FAUST real-time functional, for signal processing, compiler provides automatic parallelization via OpenMP or a specific work-stealing scheduler Fortran  coarrays and do concurrent are part of Fortran 2008 standard Go for system programming, with a concurrent programming model based on CSP Hume functional, concurrent, for bounded space and time environments where automata processes are described by synchronous channels patterns and message passing Io actor-based concurrency Janus features distinct askers and tellers to logical variables, bag channels is purely declarative Java thread class or Runnable interface Julia concurrent programming primitives Tasks, async-wait, Channels. 9  JavaScript via web workers , in a browser environment, promises , and callbacks . JoCaml concurrent and distributed channel based, extension of OCaml , implements the join-calculus of processes Join Java concurrent, based on Java language Joule dataflow-based, communicates by message passing Joyce concurrent, teaching, built on Concurrent Pascal with features from CSP by Per Brinch Hansen LabVIEW graphical, dataflow, functions are nodes in a graph, data is wires between the nodes includes object-oriented language Limbo relative of Alef , for system programming in Inferno operating system MultiLisp  Scheme variant extended to support parallelism Modula-2 for system programming, by N. Wirth as a successor to Pascal with native support for coroutines Modula-3 modern member of Algol family with extensive support for threads, mutexes, condition variables Newsqueak for research, with channels as first-class values predecessor of Alef occam influenced heavily by communicating sequential processes CSP occam- a modern variant of occam , which incorporates ideas from Milners -calculus Orc heavily concurrent, nondeterministic, based on Kleene algebra Oz-Mozart multiparadigm, supports shared-state and message-passing concurrency, and futures ParaSail object-oriented, parallel, free of pointers, race conditions Pict essentially an executable implementation of Milners -calculus Perl with AnyEvent and Coro Perl 6 includes classes for threads, promises and channels by default 10  . Pony - deadlock and race free, non-blocking-only actor language. Python with Twisted , greenlet and gevent , or using Stackless Python Reia uses asynchronous message passing between shared-nothing objects RedSystem for system programming, based on Rebol Ruby with Concurrent Ruby and Celluloid Rust for system programming, using message-passing with move semantics, shared immutable memory, and shared mutable memory. 11  SALSAactor-based with token-passing, join, and first-class continuations for distributed computing over the Internet Scala general purpose, designed to express common programming patterns in a concise, elegant, and type-safe way SequenceL general purpose functional, main design objectives are ease of programming, code clarity-readability, and automatic parallelization for performance on multicore hardware, and provably free of race conditions SR for research StratifiedJS combinator-based concurrency, based on JavaScript SuperPascal concurrent, for teaching, built on Concurrent Pascal and Joyce by Per Brinch Hansen Unicon for research Termite Scheme adds Erlang-like concurrency to Scheme TNSDL for developing telecommunication exchanges, uses asynchronous message passing VHSIC Hardware Description Language  VHDL IEEE STD-1076 XC concurrency-extended subset of C language developed by XMOS , based on communicating sequential processes , built-in constructs for programmable IO Many other languages provide support for concurrency in the form of libraries, at levels roughly comparable with the above list. See also  edit  Asynchronous IO Chu space Flow-based programming Java ConcurrentMap List of important publications in concurrent, parallel, and distributed computing Ptolemy Project Race condition   Computing Sheaf mathematics Software transactional memory Transaction processing Notes  edit   This is discounting parallelism internal to a processor core, such as pipelining or vectorized instructions. A one-core, one-processor machine may be capable of some parallelism, such as with a coprocessor , but the processor alone is not. References  edit  This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources   Concurrent computing     news    newspapers    books    scholar    JSTOR  February 2014   Learn how and when to remove this template message   Operating System Concepts 9th edition, Abraham Silberschatz. Chapter 4 Threads  a b Pike, Rob 2012-01-11. Concurrency is not Parallelism. Waza conference , 11 January 2012. Retrieved from httptalks.golang.org2012waza.slide slides and httpvimeo.com49718712 video.  Parallelism vs. Concurrency . Haskell Wiki . .mw-parser-output cite.citationfont-styleinherit.mw-parser-output .citation qquotes.mw-parser-output .citation .cs1-lock-free abackgroundurlupload.wikimedia.orgwikipediacommonsthumb665Lock-green.svg9px-Lock-green.svg.pngno-repeatbackground-positionright .1em center.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation . cs1-lock-registration abackgroundurlupload.wikimedia.orgwikipediacommonsthumbdd6Lock-gray-alt-2.svg9px-Lock-gray-alt-2.svg.pngno-repeatbackground-positionright .1em center.mw-parser-output .citation .cs1-lock-subscription abackgroundurlupload.wikimedia.orgwikipediacommonsthumbaaaLock-red-alt-2.svg9px-Lock-red-alt-2.svg.pngno-repeatbackground-po sitionright .1em center.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registrationcolor555.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration spanborder-bottom1px dottedcursorhelp.mw-parser-output .cs1-ws-icon abackgroundurlupload.wikimedia.orgwikipediacommonsthumb44cWikisource-logo.svg12px-Wikisource-logo.svg.pngno -repeatbackground-positionright .1em center.mw-parser-output code.cs1-codecolorinheritbackgroundinheritborderinheritpaddinginherit.mw-parser-output .cs1-hidden-errordisplaynonefont-size100.mw-parser-output .cs1-visible-errorfont-size100.mw-parser-output .cs1-maintdisplaynonecolor33aa33margin-left0.3em.mw-parser-output .cs1-subscription,.mw-parser-o utput .cs1-registration,.mw-parser-output .cs1-formatfont-size95.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-leftpadding-left0.2em.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-rightpadding-right0.2em  Schneider, Fred B. 1997-05-06. On Concurrent Programming . Springer. ISBN   9780387949420 .  a b Ben-Ari, Mordechai 2006. Principles of Concurrent and Distributed Programming 2nd ed.. Addison-Wesley. ISBN   978-0-321-31283-9 .  Patterson  Hennessy 2013 , p.  503.  PODC Influential Paper Award 2002 , ACM Symposium on Principles of Distributed Computing , retrieved 2009-08-24  Armstrong, Joe 2003. Making reliable distributed systems in the presence of software errors PDF .  httpsjuliacon.talkfunnel.com201521-concurrent-and-parallel-programming-in-julia Concurrent and Parallel programming in Julia  Concurrency . docs.perl6.org . Retrieved 2017-12-24 .  Blum, Ben 2012. Typesafe Shared Mutable State . Retrieved 2012-11-14 . Sources  edit  .mw-parser-output .refbeginfont-size90margin-bottom0.5em.mw-parser-output .refbegin-hanging-indentsullist-style-typenonemargin-left0.mw-parser-output .refbegin-hanging-indentsulli,.mw-parser-output .refbegin-hanging-indentsdlddmargin-left0padding-left3.2emtext-indent-3.2emlist-stylenone.mw-parser-output .refbegin-100font-size100 Patterson, David A. Hennessy, John L. 2013. Computer Organization and Design The HardwareSoftware Interface . The Morgan Kaufmann Series in Computer Architecture and Design 5 ed.. Morgan Kaufmann. ISBN   978-0-12407886-4 . Further reading  edit  Dijkstra, E. W. 1965. Solution of a problem in concurrent programming control. Communications of the ACM . 8 9 569. doi  10.1145365559.365617 . Herlihy, Maurice 2008 2008. The Art of Multiprocessor Programming . Morgan Kaufmann. ISBN   978-0123705914 . Downey, Allen B. 2005 2005. The Little Book of Semaphores PDF . Green Tea Press. ISBN   978-1-4414-1868-5 . Archived from the original PDF on 2016-03-04 . Retrieved 2009-11-21 . Cite uses deprecated parameter dead-url  help  Filman, Robert E. Daniel P. Friedman 1984. Coordinated Computing Tools and Techniques for Distributed Software . New York McGraw-Hill. p.  370. ISBN   978-0-07-022439-1 . Leppjrvi, Jouni 2008. A pragmatic, historically oriented survey on the universality of synchronization primitives PDF . University of Oulu. Taubenfeld, Gadi 2006. Synchronization Algorithms and Concurrent Programming . Pearson  Prentice Hall. p.  433. ISBN   978-0-13-197259-9 . External links  edit  Media related to Concurrent programming at Wikimedia Commons Concurrent Systems Virtual Library v t e Edsger Dijkstra Notable works A Primer of ALGOL 60 Programming book Structured Programming book A Discipline of Programming book A Method of Programming book Predicate Calculus and Program Semantics book Selected Writings on Computing A Personal Perspective book Selected papers EWD manuscripts A Note on Two Problems in Connexion with Graphs Cooperating Sequential Processes Solution of a Problem in Concurrent Programming Control The Structure of the THE-Multiprogramming System Go To Statement Considered Harmful Notes on Structured Programming The Humble Programmer Programming Considered as a Human Activity How Do We Tell Truths That Might Hurt On the Role of Scientific Thought Self-stabilizing Systems in Spite of Distributed Control On the Cruelty of Really Teaching Computer Science Main research areas Theoretical computing science Software engineering Systems science Algorithm design Concurrent computing Distributed computing Formal methods Programming methodology Programming language research Program design and development Software architecture Philosophy of computer programming and computing science Scientific contributions Concepts and methods ALGOL 60 implementation Call stack Concurrency Concurrent programming Cooperating sequential processes Critical section Deadly embrace  deadlock  Dining philosophers problem Dutch national flag problem Fault-tolerant system Goto-less programming Guarded Command Language Layered structure in software architecture Levels of abstraction Multithreaded programming Mutual exclusion  mutex  Producerconsumer problem  bounded buffer problem  Program families Predicate transformer semantics Process synchronization Self-stabilizing distributed system Semaphore programming Separation of concerns Sleeping barber problem Software crisis Structured analysis Structured programming THE multiprogramming system Unbounded nondeterminism Weakest precondition calculus Algorithms Bankers algorithm Dijkstras algorithm DJP algorithm  Prims algorithm  Dijkstra-Scholten algorithm Dekkers algorithm generalization Smoothsort Shunting-yard algorithm Tri-color marking algorithm Concurrent algorithms Distributed algorithms Deadlock prevention algorithms Mutual exclusion algorithms Self-stabilizing algorithms Related people Shlomi Dolev Per Brinch Hansen Tony Hoare Ole-Johan Dahl Leslie Lamport David Parnas Carel S. Scholten Adriaan van Wijngaarden Niklaus Wirth Other topics Dijkstra Prize  Edsger W. Dijkstra Prize in Distributed Computing  Centrum Wiskunde  Informatica E.W. Dijkstra Archive  University of Texas at Austin  List of pioneers in computer science List of important publications in computer science List of important publications in theoretical computer science List of important publications in concurrent, parallel, and distributed computing International Symposium on Stabilization, Safety, and Security of Distributed Systems Wikiquote v t e Concurrent computing General Concurrency Concurrency control Process calculi CSP CCS ACP LOTOS -calculus Ambient calculus API-Calculus PEPA Join-calculus Classic problems ABA problem Cigarette smokers problem Deadlock Dining philosophers problem Producerconsumer problem Race condition Readerswriters problem Sleeping barber problem   Category Concurrent computing v t e Types of programming languages Actor-based Array Aspect-oriented Class-based Concatenative Concurrent Dataflow Declarative Domain-specific Dynamic Esoteric Event-driven Extensible Functional Imperative Logic Macro Metaprogramming Multi-paradigm Object-based Object-oriented Pipeline Procedural Prototype-based Reflective Rule-based Scripting Stack based Synchronous Tactile Templating Assembly Compiled Interpreted Machine Low-level High-level Very high-level First generation Second generation Third generation Fourth generation Fifth generation Non-English-based Visual Retrieved from  httpsen.wikipedia.orgwindex.phptitleConcurrent_computing oldid910062222  Categories  Operating system technology Concurrent computing Edsger W. Dijkstra Dutch inventions Hidden categories Articles needing additional references from December 2016 All articles needing additional references Articles that may contain original research from December 2016 All articles that may contain original research Articles with multiple maintenance issues All articles with unsourced statements Articles with unsourced statements from December 2016 Articles needing additional references from December 2006 Articles to be expanded from February 2014 All articles to be expanded Articles using small message boxes Articles with unsourced statements from May 2013 Articles with unsourced statements from August 2010 Articles needing additional references from February 2014 CS1 errors deprecated parameters Commons category link is on Wikidata